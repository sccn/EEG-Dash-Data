{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ds005514 and start training (see below to read presaved data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records:  20\n",
      "first record:  {'_id': ObjectId('6789a3c766ea5ef303140d0c'), 'data_name': 'ds005514_sub-NDARRW481GFE_task-RestingState_eeg.set', 'dataset': 'ds005514', 'bidspath': 'ds005514/sub-NDARRW481GFE/eeg/sub-NDARRW481GFE_task-RestingState_eeg.set', 'subject': 'NDARRW481GFE', 'task': 'RestingState', 'session': '', 'run': '', 'sampling_frequency': 500, 'modality': 'EEG', 'channel_names': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20', 'E21', 'E22', 'E23', 'E24', 'E25', 'E26', 'E27', 'E28', 'E29', 'E30', 'E31', 'E32', 'E33', 'E34', 'E35', 'E36', 'E37', 'E38', 'E39', 'E40', 'E41', 'E42', 'E43', 'E44', 'E45', 'E46', 'E47', 'E48', 'E49', 'E50', 'E51', 'E52', 'E53', 'E54', 'E55', 'E56', 'E57', 'E58', 'E59', 'E60', 'E61', 'E62', 'E63', 'E64', 'E65', 'E66', 'E67', 'E68', 'E69', 'E70', 'E71', 'E72', 'E73', 'E74', 'E75', 'E76', 'E77', 'E78', 'E79', 'E80', 'E81', 'E82', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'E90', 'E91', 'E92', 'E93', 'E94', 'E95', 'E96', 'E97', 'E98', 'E99', 'E100', 'E101', 'E102', 'E103', 'E104', 'E105', 'E106', 'E107', 'E108', 'E109', 'E110', 'E111', 'E112', 'E113', 'E114', 'E115', 'E116', 'E117', 'E118', 'E119', 'E120', 'E121', 'E122', 'E123', 'E124', 'E125', 'E126', 'E127', 'E128', 'Cz'], 'channel_types': ['EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG'], 'nchans': 129, 'ntimes': 174586, 'participantinfo': {'age': 5.7012, 'gender': 'M'}, 'channel_tsv': {'name': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20', 'E21', 'E22', 'E23', 'E24', 'E25', 'E26', 'E27', 'E28', 'E29', 'E30', 'E31', 'E32', 'E33', 'E34', 'E35', 'E36', 'E37', 'E38', 'E39', 'E40', 'E41', 'E42', 'E43', 'E44', 'E45', 'E46', 'E47', 'E48', 'E49', 'E50', 'E51', 'E52', 'E53', 'E54', 'E55', 'E56', 'E57', 'E58', 'E59', 'E60', 'E61', 'E62', 'E63', 'E64', 'E65', 'E66', 'E67', 'E68', 'E69', 'E70', 'E71', 'E72', 'E73', 'E74', 'E75', 'E76', 'E77', 'E78', 'E79', 'E80', 'E81', 'E82', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'E90', 'E91', 'E92', 'E93', 'E94', 'E95', 'E96', 'E97', 'E98', 'E99', 'E100', 'E101', 'E102', 'E103', 'E104', 'E105', 'E106', 'E107', 'E108', 'E109', 'E110', 'E111', 'E112', 'E113', 'E114', 'E115', 'E116', 'E117', 'E118', 'E119', 'E120', 'E121', 'E122', 'E123', 'E124', 'E125', 'E126', 'E127', 'E128', 'Cz'], 'type': ['EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG'], 'units': ['uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV', 'uV']}, 'eeg_json': {'TaskName': 'seqLearning6target', 'SamplingFrequency': 500, 'PowerLineFrequency': 60, 'EEGReference': 'Cz', 'RecordingType': 'continuous', 'SoftwareFilters': 'n/a', 'InstitutionName': 'Child Mind Institute', 'InstitutionAddress': '101 E 56th St, New York, NY 10022', 'Manufacturer': 'Magtism EGI', 'ManufacturersModelName': '128-channel GSN 200 v.2.1', 'TaskDescription': \"The visual geometric stimuli consisted of black symbols with a size of 1 cm width and 1 cm height. As on each page of the paper version, 15 trials were presented at a time on the screen. Each row contained two target symbols and five search symbols, arranged horizontally across the row. Participants were instructed to indicate for each row, by mouse-click (mark either the yes or no checkbox), whether either of the target symbols matched with any of the five search symbols. The participants had the option to correct their initial responses if they desired. Participants were instructed to solve as many rows, or trials, as possible within two minutes. Before beginning the actual paradigm, participants performed a training block with 4 trials, for which they received feedback, to ensure their comprehension of the task. No feedback was provided throughout the actual task. Once a participant finished all 15 trials, they pressed the 'next page' button to advance onward. There were 4 pages (a maximum of 60 trials) in total. No participant ever reached the end of the 60 trials.\", 'Instructions': 'The task is to figure out if either one of the two first symbols are presented again in the same line. Press with the left mouse button YES and NO boxes to select your answer. If you accidently press the wrong button you can make a correction by simply clicking on the other response. You have 2 min to solve as many trials as possible.', 'EEGChannelCount': 129, 'RecordingDuration': 265.078}, 'participant_tsv': {'release_number': 'R9', 'sex': 'M', 'age': 5.7012, 'ehq_total': 86.67, 'commercial_use': 'Yes', 'full_pheno': 'Yes', 'p_factor': 1.3019999999999998, 'attention': -2.086, 'internalizing': -0.264, 'externalizing': 0.327, 'RestingState': 'available', 'DespicableMe': 'available', 'FunwithFractals': 'available', 'ThePresent': 'available', 'DiaryOfAWimpyKid': 'available', 'contrastChangeDetection_1': 'available', 'contrastChangeDetection_2': 'caution', 'contrastChangeDetection_3': 'caution', 'surroundSupp_1': 'available', 'surroundSupp_2': 'available', 'seqLearning6target': 'available', 'seqLearning8target': 'unavailable', 'symbolSearch': 'available'}, 'rawdatainfo': {'sampling_frequency': 500, 'nchans': 129, 'ntimes': 132539, 'channel_types': ['EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG', 'EEG'], 'channel_names': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20', 'E21', 'E22', 'E23', 'E24', 'E25', 'E26', 'E27', 'E28', 'E29', 'E30', 'E31', 'E32', 'E33', 'E34', 'E35', 'E36', 'E37', 'E38', 'E39', 'E40', 'E41', 'E42', 'E43', 'E44', 'E45', 'E46', 'E47', 'E48', 'E49', 'E50', 'E51', 'E52', 'E53', 'E54', 'E55', 'E56', 'E57', 'E58', 'E59', 'E60', 'E61', 'E62', 'E63', 'E64', 'E65', 'E66', 'E67', 'E68', 'E69', 'E70', 'E71', 'E72', 'E73', 'E74', 'E75', 'E76', 'E77', 'E78', 'E79', 'E80', 'E81', 'E82', 'E83', 'E84', 'E85', 'E86', 'E87', 'E88', 'E89', 'E90', 'E91', 'E92', 'E93', 'E94', 'E95', 'E96', 'E97', 'E98', 'E99', 'E100', 'E101', 'E102', 'E103', 'E104', 'E105', 'E106', 'E107', 'E108', 'E109', 'E110', 'E111', 'E112', 'E113', 'E114', 'E115', 'E116', 'E117', 'E118', 'E119', 'E120', 'E121', 'E122', 'E123', 'E124', 'E125', 'E126', 'E127', 'E128', 'Cz']}}\n"
     ]
    }
   ],
   "source": [
    "from eegdash import EEGDash\n",
    "from eegdash.data_utils import EEGDashBaseRaw\n",
    "\n",
    "eegdashdata = EEGDash(is_public=False)\n",
    "records = eegdashdata.findrecord({'dataset': 'ds005514', 'task': 'RestingState'})\n",
    "records = records[0:20]\n",
    "\n",
    "print(\"number of records: \", len(records))\n",
    "print(\"first record: \", records[0]) # current call\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arno/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/braindecode/datasets/base.py:131: UserWarning: 'genderbin' not in description. '__getitem__'will fail unless an appropriate target is added to description.\n",
      "  warnings.warn(f\"'{name}' not in description. '__getitem__'\"\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datasets import BaseDataset, BaseConcatDataset\n",
    "gender_to_label = {'M': int(0), 'F': int(1)}\n",
    "\n",
    "allEEGDash = []\n",
    "for record in records:\n",
    "    s3_path = eegdashdata.get_s3path(record)\n",
    "    sfreq = record['sampling_frequency']\n",
    "    nchans = record['nchans']\n",
    "    ntimes = record['ntimes']\n",
    "    ch_names = record['channel_names']\n",
    "    ch_types = record['channel_types']\n",
    "    s3_path = eegdashdata.get_s3path(record)\n",
    "    eegdash = BaseDataset(EEGDashBaseRaw(s3_path, {'sfreq': sfreq, 'nchans': nchans, 'n_times': ntimes, 'ch_types': ch_types, 'ch_names': ch_names}, preload=False), target_name='genderbin')\n",
    "    eegdash.set_description({'age': record['participantinfo']['age'], 'gender': record['participantinfo']['gender'], 'genderbin': gender_to_label[record['participantinfo']['gender']], 'subject': record['subject']})\n",
    "    allEEGDash.append(eegdash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegdash_braindecode = BaseConcatDataset(allEEGDash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>genderbin</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.7012</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARRW481GFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.5951</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARTE553VC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0696</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARFJ488VPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.7882</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARHT403JJJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.5762</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARYW170CAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0025</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARYE017HZA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.1071</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARXX426ELJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.2295</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARUZ818ADU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.1426</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARZZ741VJE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.9038</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARAW216PM7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.8661</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARDY421HTK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.7063</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARZW854BH0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.7650</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARRR464UTB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.1887</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARDL377ZT7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.7328</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARMZ936WYB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.3090</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARGC407NV7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.3044</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARDB033FW5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12.3219</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NDARNA055LLN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.8350</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARAU939WUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.6178</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>NDARRU751ATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age gender  genderbin       subject\n",
       "0    5.7012      M          0  NDARRW481GFE\n",
       "1   13.5951      M          0  NDARTE553VC1\n",
       "2   12.0696      F          1  NDARFJ488VPT\n",
       "3   10.7882      F          1  NDARHT403JJJ\n",
       "4   12.5762      M          0  NDARYW170CAA\n",
       "5   10.0025      F          1  NDARYE017HZA\n",
       "6    6.1071      M          0  NDARXX426ELJ\n",
       "7    7.2295      M          0  NDARUZ818ADU\n",
       "8    6.1426      M          0  NDARZZ741VJE\n",
       "9    9.9038      M          0  NDARAW216PM7\n",
       "10  13.8661      M          0  NDARDY421HTK\n",
       "11  10.7063      M          0  NDARZW854BH0\n",
       "12   8.7650      M          0  NDARRR464UTB\n",
       "13  14.1887      M          0  NDARDL377ZT7\n",
       "14   7.7328      F          1  NDARMZ936WYB\n",
       "15   6.3090      M          0  NDARGC407NV7\n",
       "16  17.3044      M          0  NDARDB033FW5\n",
       "17  12.3219      F          1  NDARNA055LLN\n",
       "18   6.8350      M          0  NDARAU939WUK\n",
       "19  15.6178      M          0  NDARRU751ATE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "res = eegdash_braindecode.description\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 4.096 (s)\n",
      "Plotting power spectral density (dB=True).\n",
      "sampling_freq: 500.0\n",
      "(array([[ 2402.36499023,  2358.11035156,  2292.43237305, ...,\n",
      "        -3395.13867188, -3489.63623047, -3587.44360352]]), array([ 10.   ,  10.002,  10.004, ..., 445.466, 445.468, 445.47 ]))\n",
      "-3617.61279296875 4021.9267578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arno/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/mne/viz/utils.py:158: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  (fig or plt).show(**kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<MNELineFigure size 1000x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "raw = eegdash_braindecode.datasets[1].raw\n",
    "fig = raw.compute_psd(fmax=70).plot(\n",
    "    average=True, amplitude=False, picks=\"data\", exclude=\"bads\"\n",
    ")\n",
    "sampling_freq = raw.info[\"sfreq\"]\n",
    "print('sampling_freq:', sampling_freq)\n",
    "start_stop_seconds = np.array([10, 500])\n",
    "start_sample, stop_sample = (start_stop_seconds * sampling_freq).astype(int)\n",
    "channel_index = 0\n",
    "raw_selection = raw[channel_index, start_sample:stop_sample]\n",
    "print(raw_selection)\n",
    "x = raw_selection[1]\n",
    "y = raw_selection[0].T\n",
    "print(np.min(y),np.max(y))\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/r_x6gj790wvcx_mh157ctypm0000gn/T/ipykernel_56617/378209447.py:5: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab('.eegdash_cache/sub-NDARAU939WUK_task-RestingState_eeg.set', preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 55 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 55.00 Hz\n",
      "- Upper transition bandwidth: 9.00 Hz (-6 dB cutoff frequency: 59.50 Hz)\n",
      "- Filter length: 423 samples (3.305 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from braindecode.preprocessing import (preprocess, Preprocessor, create_fixed_length_windows)\n",
    "\n",
    "# check channel selections\n",
    "raw = mne.io.read_raw_eeglab('.eegdash_cache/sub-NDARAU939WUK_task-RestingState_eeg.set', preload=True)\n",
    "chan_labels = ['E22', 'E9', 'E33', 'E24', 'E11', 'E124', 'E122', 'E29', 'E6', 'E111', 'E45', 'E36', 'E104', 'E108', 'E42', 'E55', 'E93', 'E58', 'E52', 'E62', 'E92', 'E96', 'E70', 'Cz']\n",
    "raw.pick_channels(chan_labels) #raw.get_montage().ch_names\n",
    "\n",
    "# Alternatively, if you want to include this as a preprocessing step in a Braindecode pipeline:\n",
    "preprocessors = [\n",
    "    Preprocessor('pick_channels', ch_names=chan_labels),\n",
    "    Preprocessor(\"resample\", sfreq=128),\n",
    "    Preprocessor(\"filter\", l_freq=1, h_freq=55)\n",
    "]\n",
    "preprocess(eegdash_braindecode, preprocessors, n_jobs=-1) #, save_dir='xxxx'' will save and set preload to false\n",
    "\n",
    "windows_ds = create_fixed_length_windows(eegdash_braindecode, start_offset_samples=0, stop_offset_samples=None,\n",
    "        window_size_samples=256,\n",
    "        window_stride_samples=256, drop_last_window=True,\n",
    "        preload=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/0/0-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/0/0-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/1/1-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/1/1-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/2/2-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/2/2-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/3/3-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/3/3-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/4/4-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/4/4-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/5/5-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/5/5-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/6/6-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/6/6-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/7/7-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/7/7-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/8/8-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/8/8-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/9/9-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/9/9-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/10/10-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/10/10-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/11/11-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/11/11-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/12/12-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/12/12-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/13/13-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/13/13-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/14/14-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/14/14-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/15/15-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/15/15-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/16/16-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/16/16-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/17/17-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/17/17-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/18/18-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/18/18-raw.fif\n",
      "[done]\n",
      "Writing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/19/19-raw.fif\n",
      "Closing /Users/arno/Python/EEG-Dash-Data/tests/data/hbn_preprocessed_restingstate/19/19-raw.fif\n",
      "[done]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# save all the data\n",
    "if True:\n",
    "    os.makedirs('data/hbn_preprocessed_restingstate', exist_ok=True)\n",
    "    windows_ds.save('data/hbn_preprocessed_restingstate', overwrite=True)\n",
    "else:\n",
    "    print('NOTHING WAS DONE. CHANGE THE CONDITIONAL STATEMENT.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here to load presaved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if windows_ds is a variable in the workspace and delete it\n",
    "if 'windows_ds' in locals():\n",
    "    del windows_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from disk\n"
     ]
    }
   ],
   "source": [
    "# if you run this cell as it loads all the data from disk\n",
    "\n",
    "import os\n",
    "from braindecode.datautil import load_concat_dataset\n",
    "\n",
    "print(\"Loading data from disk\")\n",
    "windows_ds = load_concat_dataset(path='data/hbn_preprocessed_restingstate', preload=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_ds[0][0]: (24, 256)\n",
      "Number of samples - Train: 1204, Val: 349, Test: 376\n",
      "Train gender balance: 0.50\n",
      "Val gender balance: 0.50\n",
      "Test gender balance: 0.50\n"
     ]
    }
   ],
   "source": [
    "from braindecode.datasets import BaseDataset, BaseConcatDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "subject_genders = windows_ds.description['genderbin']\n",
    "random_state = 42\n",
    "\n",
    "# Get balanced indices for male and female subjects\n",
    "male_indices = np.where(subject_genders == 0)[0]\n",
    "female_indices = np.where(subject_genders == 1)[0]\n",
    "n_samples = min(len(male_indices), len(female_indices))\n",
    "balanced_indices = np.concatenate([male_indices[:n_samples], female_indices[:n_samples]])\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "train_val_indices, test_indices = train_test_split(balanced_indices, test_size=0.2, stratify=subject_genders[balanced_indices], random_state=random_state)\n",
    "\n",
    "# Second split: separate train and validation sets (75% train, 25% validation of remaining data)\n",
    "# This gives approximately 60/20/20 split of the original data\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.25, stratify=subject_genders[train_val_indices],random_state=random_state)\n",
    "\n",
    "# Create datasets\n",
    "train_ds = BaseConcatDataset([windows_ds.datasets[i] for i in train_indices])\n",
    "val_ds = BaseConcatDataset([windows_ds.datasets[i] for i in val_indices])\n",
    "test_ds = BaseConcatDataset([windows_ds.datasets[i] for i in test_indices])\n",
    "\n",
    "# train_ds2 = [[ds[0].reshape(1, ds[0].shape[0], ds[0].shape[1]), ds[1], ds[2]] for ds in train_ds]\n",
    "# test_ds2  = [[ds[0].reshape(1, ds[0].shape[0], ds[0].shape[1]), ds[1], ds[2]] for ds in test_ds]\n",
    "# print(\"Shape of train_ds2[0][0]:\", train_ds2[0][0].shape)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=100, shuffle=True)\n",
    "val_loader =  DataLoader(val_ds, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=100, shuffle=True)\n",
    "\n",
    "# Print shapes and sizes to verify split\n",
    "print(\"Shape of train_ds[0][0]:\", train_ds[0][0].shape)\n",
    "print(f\"Number of samples - Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "print(f\"Train gender balance: {np.mean(train_ds.description['genderbin']):.2f}\")\n",
    "print(f\"Val gender balance: {np.mean(val_ds.description['genderbin']):.2f}\")\n",
    "print(f\"Test gender balance: {np.mean(test_ds.description['genderbin']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "first_item, label, sz = dataiter.__next__() # question why is the label lost here \n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import torch\n",
    "\n",
    "path = './data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "logger = Logger(mode='debug')\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "if USE_GPU and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, loss = 0.6934\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 0, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 1, Iteration 0, loss = 0.6946\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 1, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 2, Iteration 0, loss = 0.6951\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 2, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 3, Iteration 0, loss = 0.6955\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 3, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 4, Iteration 0, loss = 0.6922\n",
      "Got 595 / 1204 correct (49.42)\n",
      "Got 174 / 349 correct (49.86)\n",
      "Epoch 4, Train accuracy: 0.4941860465116279, Validation accuracy: 0.498567335243553\n",
      "Epoch 5, Iteration 0, loss = 0.6934\n",
      "Got 595 / 1204 correct (49.42)\n",
      "Got 174 / 349 correct (49.86)\n",
      "Epoch 5, Train accuracy: 0.4941860465116279, Validation accuracy: 0.498567335243553\n",
      "Epoch 6, Iteration 0, loss = 0.6946\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 6, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 7, Iteration 0, loss = 0.6928\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 7, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 8, Iteration 0, loss = 0.6923\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 8, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n",
      "Epoch 9, Iteration 0, loss = 0.6921\n",
      "Got 609 / 1204 correct (50.58)\n",
      "Got 175 / 349 correct (50.14)\n",
      "Epoch 9, Train accuracy: 0.5058139534883721, Validation accuracy: 0.501432664756447\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(mode='debug')\n",
    "model = create_model('vgg','raw')\n",
    "\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.002, weight_decay=0.001)\n",
    "model = train(model, optimizer, 10, logger, device, dtype, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 24, 614]              [1, 2]                    --                        --\n",
      "├─Ensure4d (ensuredims): 1-1             [1, 24, 614]              [1, 24, 614, 1]           --                        --\n",
      "├─Rearrange (dimshuffle): 1-2            [1, 24, 614, 1]           [1, 1, 614, 24]           --                        --\n",
      "├─CombinedConv (conv_time_spat): 1-3     [1, 1, 614, 24]           [1, 40, 590, 1]           39,440                    --\n",
      "├─BatchNorm2d (bnorm): 1-4               [1, 40, 590, 1]           [1, 40, 590, 1]           80                        --\n",
      "├─Expression (conv_nonlin_exp): 1-5      [1, 40, 590, 1]           [1, 40, 590, 1]           --                        --\n",
      "├─AvgPool2d (pool): 1-6                  [1, 40, 590, 1]           [1, 40, 35, 1]            --                        [75, 1]\n",
      "├─Expression (pool_nonlin_exp): 1-7      [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
      "├─Dropout (drop): 1-8                    [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-9          [1, 40, 35, 1]            [1, 2]                    --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 35, 1]            [1, 2, 1, 1]              2,802                     [35, 1]\n",
      "│    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
      "│    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
      "============================================================================================================================================\n",
      "Total params: 42,322\n",
      "Trainable params: 42,322\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.19\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.26\n",
      "============================================================================================================================================\n",
      "Model device: mps:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arno/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/braindecode/models/base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
      "  warnings.warn(\n",
      "/Users/arno/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet\n",
    "from torchinfo import summary\n",
    "\n",
    "# re-import models_dung_2021\n",
    "#import importlib\n",
    "#import models_dung_2021\n",
    "#importlib.reload(models_dung_2021)  # This ensures we get the latest version\n",
    "# from models_dung_2021 import (create_model_original_129_614, create_model_vgg16)\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if CUDA GPU is available\n",
    "mps = torch.backends.mps.is_available()  # check if MPS (Apple GPU) is available\n",
    "\n",
    "if cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed = 20200220\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes = 2\n",
    "classes = list(range(n_classes))\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_channels = windows_ds[0][0].shape[0]\n",
    "input_window_samples = windows_ds[0][0].shape[1]\n",
    "\n",
    "model_name = 'shallow' #'shallow' #'original' #'vgg16'\n",
    "model = ShallowFBCSPNet(\n",
    "    n_channels,\n",
    "    n_classes,\n",
    "    input_window_samples=input_window_samples,\n",
    "    final_conv_length=\"auto\",\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "model.to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5.5133, Val: 0.5000\n",
      "Epoch [2/20], Loss: 5.2003, Val: 0.5000\n",
      "Epoch [3/20], Loss: 7.0930, Val: 0.5000\n",
      "Epoch [4/20], Loss: 6.5474, Val: 0.5000\n",
      "Epoch [5/20], Loss: 4.0178, Val: 0.5000\n",
      "Epoch [6/20], Loss: 5.4671, Val: 0.5000\n",
      "Epoch [7/20], Loss: 6.1477, Val: 0.5000\n",
      "Epoch [8/20], Loss: 4.9943, Val: 0.5000\n",
      "Epoch [9/20], Loss: 4.8296, Val: 0.5000\n",
      "Epoch [10/20], Loss: 4.7546, Val: 0.5000\n",
      "Epoch [11/20], Loss: 5.5438, Val: 0.5000\n",
      "Epoch [12/20], Loss: 4.5574, Val: 0.5000\n",
      "Epoch [13/20], Loss: 5.2199, Val: 0.5000\n",
      "Epoch [14/20], Loss: 5.6478, Val: 0.5000\n",
      "Epoch [15/20], Loss: 5.0804, Val: 0.5000\n",
      "Epoch [16/20], Loss: 3.5895, Val: 0.5000\n",
      "Epoch [17/20], Loss: 4.1040, Val: 0.5000\n",
      "Epoch [18/20], Loss: 5.1488, Val: 0.5000\n",
      "Epoch [19/20], Loss: 5.9807, Val: 0.5000\n",
      "Epoch [20/20], Loss: 5.5329, Val: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming 'model' is your ShallowFBCSPNet instance\n",
    "# Assuming 'dataloader' is your DataLoader instance\n",
    "def normalize_data(data):\n",
    "    return (data - data.mean()) / (data.std() + 1e-8)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20  # Number of epochs\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target, sz) in enumerate(train_loader):\n",
    "        # Move data to the appropriate device (e.g., GPU if available)\n",
    "        data = normalize_data(data)\n",
    "        if model_name == 'vgg16' or model_name == 'original':\n",
    "            data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "        # print(f\"Data device: {data.device}, Data type: {data.dtype}\")\n",
    "        # print(f\"Target device: {target.device}, Target type: {target.dtype}\")        \n",
    "        # print(data.shape)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = F.cross_entropy(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = np.array([])\n",
    "    total = 0\n",
    "    for batch_idx, (data, target, sz) in enumerate(val_loader):\n",
    "        data = normalize_data(data)\n",
    "        if model_name == 'vgg16' or model_name == 'original':\n",
    "            data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct = np.concatenate((correct, (predicted == target).to(\"cpu\").numpy()))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val: {np.mean(correct):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 336.32 MB\n",
      "Cached: 29802.00 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def find_max_batch_size(model, max_batch_size=1024):\n",
    "    batch_size = 1\n",
    "    while batch_size <= max_batch_size:\n",
    "        try:\n",
    "            # Create dummy input\n",
    "            x = torch.randn(batch_size, 1, n_channels, input_window_samples, device='cuda')\n",
    "            output = model(x)\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_size *= 2\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                return batch_size // 2\n",
    "            raise e\n",
    "    return batch_size // 2\n",
    "\n",
    "optimal_batch_size = find_max_batch_size(model)\n",
    "print(f\"Optimal batch size: {optimal_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "tmp = (predicted == target).to(\"cpu\").numpy()\n",
    "correct = np.concatenate((correct, tmp))\n",
    "# append all elements of tmp to correct\n",
    "#[correct.append(tmp2) for tmp2 in tmp]\n",
    "#correct\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.50%\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = []\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for data, target in test_loader2:\n",
    "        data = normalize_data(data)\n",
    "        if model_name == 'vgg16' or model_name == 'original':\n",
    "            data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct.append((predicted == target).sum().item())\n",
    "\n",
    "print(f\"Accuracy: {100 * np.sum(correct) / total:.2f}%\")\n",
    "\n",
    "# compute bootstrap and 95% confidence interval for correct\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 37.47%\n",
      "95% CI: [30.00%, 45.07%]\n"
     ]
    }
   ],
   "source": [
    "# compute bootstrap and 95% confidence interval for correct\n",
    "n_bootstrap = 1000\n",
    "bootstrap_accuracies = []\n",
    "\n",
    "for _ in range(n_bootstrap):\n",
    "    # Sample with replacement from correct list\n",
    "    bootstrap_sample = np.random.choice(correct, size=len(correct), replace=True)\n",
    "    bootstrap_acc = 100 * np.sum(bootstrap_sample) / total\n",
    "    bootstrap_accuracies.append(bootstrap_acc)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "ci_lower = np.percentile(bootstrap_accuracies, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_accuracies, 97.5)\n",
    "mean_acc = np.mean(bootstrap_accuracies)\n",
    "\n",
    "print(f\"Mean Accuracy: {mean_acc:.2f}%\")\n",
    "print(f\"95% CI: [{ci_lower:.2f}%, {ci_upper:.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage for batch_size=256:\n",
      "Input tensors: 0.08 GB\n",
      "Model parameters: 0.06 GB\n",
      "Gradients: 0.06 GB\n",
      "Optimizer states: 0.13 GB\n",
      "Activation maps: 0.23 GB\n",
      "Computation buffers: 0.15 GB\n",
      "Total estimated: 0.71 GB\n",
      "Batch size 256 should fit in memory\n"
     ]
    }
   ],
   "source": [
    "def calculate_batch_memory(model, batch_size, input_shape, precision=32):\n",
    "    \"\"\"\n",
    "    Calculate memory usage without allocating memory\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        batch_size: int\n",
    "        input_shape: tuple of input dimensions (excluding batch)\n",
    "        precision: bit precision (16 or 32)\n",
    "    \n",
    "    Returns:\n",
    "        Memory usage in GB\n",
    "    \"\"\"\n",
    "    bytes_per_element = precision / 8  # Convert bits to bytes\n",
    "    \n",
    "    # Input size\n",
    "    input_size = batch_size * np.prod(input_shape) * bytes_per_element\n",
    "    \n",
    "    # Model parameters\n",
    "    param_size = sum(p.numel() for p in model.parameters()) * bytes_per_element\n",
    "    \n",
    "    # Gradients (same size as parameters during training)\n",
    "    gradient_size = param_size\n",
    "    \n",
    "    # Optimizer states (assumes Adam - 2 states per parameter)\n",
    "    optimizer_size = param_size * 2\n",
    "    \n",
    "    # Estimate activation maps (this is approximate and architecture-dependent)\n",
    "    # Typically 2-3x input size for most architectures\n",
    "    activation_size = input_size * 3\n",
    "    \n",
    "    # Buffer for intermediate computations (rough estimate)\n",
    "    buffer_size = input_size * 2\n",
    "    \n",
    "    total_bytes = (input_size + param_size + gradient_size + \n",
    "                  optimizer_size + activation_size + buffer_size)\n",
    "    \n",
    "    total_gb = total_bytes / (1024**3)  # Convert to GB\n",
    "    \n",
    "    print(f\"Estimated memory usage for batch_size={batch_size}:\")\n",
    "    print(f\"Input tensors: {input_size/1024**3:.2f} GB\")\n",
    "    print(f\"Model parameters: {param_size/1024**3:.2f} GB\")\n",
    "    print(f\"Gradients: {gradient_size/1024**3:.2f} GB\")\n",
    "    print(f\"Optimizer states: {optimizer_size/1024**3:.2f} GB\")\n",
    "    print(f\"Activation maps: {activation_size/1024**3:.2f} GB\")\n",
    "    print(f\"Computation buffers: {buffer_size/1024**3:.2f} GB\")\n",
    "    print(f\"Total estimated: {total_gb:.2f} GB\")\n",
    "    \n",
    "    return total_gb\n",
    "\n",
    "# Calculate for your model\n",
    "input_shape = (1, 129, 614)  # Adjust based on your model\n",
    "gpu_memory = 24  # Your GPU memory in GB\n",
    "batch_size = 256  # Start with a reasonable batch size\n",
    "\n",
    "memory_usage = calculate_batch_memory(model, batch_size, input_shape)\n",
    "if memory_usage < gpu_memory * 0.8:  # Leave 20% margin for safety\n",
    "    print(f\"Batch size {batch_size} should fit in memory\")\n",
    "else:\n",
    "    print(f\"Batch size {batch_size} might be too large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 10.6608\n",
      "Epoch [2/10], Loss: 12.4258\n",
      "Epoch [3/10], Loss: 10.7558\n",
      "Epoch [4/10], Loss: 8.3864\n",
      "Epoch [5/10], Loss: 7.6230\n",
      "Epoch [6/10], Loss: 12.7296\n",
      "Epoch [7/10], Loss: 11.7815\n",
      "Epoch [8/10], Loss: 10.1884\n",
      "Epoch [9/10], Loss: 9.6745\n",
      "Epoch [10/10], Loss: 9.3067\n",
      "Validation accuracy: 58.62%\n",
      "Epoch [1/10], Loss: 10.4266\n",
      "Epoch [2/10], Loss: 9.3364\n",
      "Epoch [3/10], Loss: 8.4209\n",
      "Epoch [4/10], Loss: 12.1507\n",
      "Epoch [5/10], Loss: 11.4463\n",
      "Epoch [6/10], Loss: 9.6799\n",
      "Epoch [7/10], Loss: 9.9175\n",
      "Epoch [8/10], Loss: 9.1513\n",
      "Epoch [9/10], Loss: 6.6600\n",
      "Epoch [10/10], Loss: 11.8379\n",
      "Validation accuracy: 55.17%\n",
      "Epoch [1/10], Loss: 10.6581\n",
      "Epoch [2/10], Loss: 10.8381\n",
      "Epoch [3/10], Loss: 9.5288\n",
      "Epoch [4/10], Loss: 9.4913\n",
      "Epoch [5/10], Loss: 9.1316\n",
      "Epoch [6/10], Loss: 8.7235\n",
      "Epoch [7/10], Loss: 9.6968\n",
      "Epoch [8/10], Loss: 10.1474\n",
      "Epoch [9/10], Loss: 11.5412\n",
      "Epoch [10/10], Loss: 9.3786\n",
      "Validation accuracy: 54.39%\n",
      "Epoch [1/10], Loss: 7.2465\n",
      "Epoch [2/10], Loss: 9.6884\n",
      "Epoch [3/10], Loss: 6.2793\n",
      "Epoch [4/10], Loss: 9.5604\n",
      "Epoch [5/10], Loss: 9.4379\n",
      "Epoch [6/10], Loss: 10.0381\n",
      "Epoch [7/10], Loss: 10.0805\n",
      "Epoch [8/10], Loss: 9.5640\n",
      "Epoch [9/10], Loss: 7.5017\n",
      "Epoch [10/10], Loss: 10.3952\n",
      "Validation accuracy: 42.11%\n",
      "Epoch [1/10], Loss: 12.5914\n",
      "Epoch [2/10], Loss: 7.9248\n",
      "Epoch [3/10], Loss: 10.0674\n",
      "Epoch [4/10], Loss: 10.4913\n",
      "Epoch [5/10], Loss: 7.4567\n",
      "Epoch [6/10], Loss: 10.1771\n",
      "Epoch [7/10], Loss: 10.8075\n",
      "Epoch [8/10], Loss: 9.5648\n",
      "Epoch [9/10], Loss: 8.7195\n",
      "Epoch [10/10], Loss: 10.6480\n",
      "Validation accuracy: 47.37%\n"
     ]
    }
   ],
   "source": [
    "# perform cross-validation randonly selectings windows_ds among len(windows_ds)\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "               \n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "for train_index, val_index in kf.split(windows_ds):\n",
    "    X = torch.FloatTensor(np.array([windows_ds[i][0] for i in train_index]))  # Convert list of arrays to single tensor\n",
    "    y = torch.LongTensor(np.array(windows_ds.get_metadata()['target']))     # Convert targets to tensor\n",
    "    dataset_train = TensorDataset(X, y[train_index])\n",
    "    dataloader_train = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    X = torch.FloatTensor(np.array([windows_ds[i][0] for i in val_index]))  # Convert list of arrays to single tensor\n",
    "    y = torch.LongTensor(np.array(windows_ds.get_metadata()['target']))     # Convert targets to tensor\n",
    "    dataset_val = TensorDataset(X, y[val_index])\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size=32, shuffle=True)\n",
    "\n",
    "    # reset model parameters\n",
    "    model = copy.deepcopy(model_copy)\n",
    "    \n",
    "    # Training loop\n",
    "    n_epochs = 10  # Number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader_train):\n",
    "            # Move data to the appropriate device (e.g., GPU if available)\n",
    "            data = normalize_data(data)\n",
    "            if model_name == 'vgg16' or model_name == 'original':\n",
    "                data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.cross_entropy(outputs, target)\n",
    "                \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        #print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_loader2):.4f}\")    \n",
    "        \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader_val):\n",
    "        data = normalize_data(data)\n",
    "        if model_name == 'vgg16' or model_name == 'original':\n",
    "            data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "            \n",
    "    print(f\"Validation accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  13.957573 ,   -2.556084 ,   27.822443 , ...,    7.5104256,\n",
       "           -7.5957413,   12.91138  ],\n",
       "        [  87.06689  ,  -79.744804 ,   95.761734 , ...,  -74.99222  ,\n",
       "           73.82263  ,  -65.78684  ],\n",
       "        [-149.94324  ,  151.19386  , -141.35626  , ...,  178.22064  ,\n",
       "         -180.66707  ,  188.04073  ],\n",
       "        ...,\n",
       "        [  58.14723  ,  -56.004196 ,   86.285164 , ...,  -84.40123  ,\n",
       "           84.80968  , -100.70467  ],\n",
       "        [  66.39647  ,  -51.849937 ,   97.117775 , ...,  -88.897766 ,\n",
       "           92.31235  , -101.10488  ],\n",
       "        [  66.11     ,  -52.419735 ,   94.67511  , ...,  -86.46298  ,\n",
       "           89.63176  ,  -99.49088  ]], dtype=float32),\n",
       " 0,\n",
       " [40, 40000, 40614])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_ds[84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  13.957573 ,   -2.556084 ,   27.822443 , ...,    7.5104256,\n",
       "           -7.5957413,   12.91138  ],\n",
       "        [  87.06689  ,  -79.744804 ,   95.761734 , ...,  -74.99222  ,\n",
       "           73.82263  ,  -65.78684  ],\n",
       "        [-149.94324  ,  151.19386  , -141.35626  , ...,  178.22064  ,\n",
       "         -180.66707  ,  188.04073  ],\n",
       "        ...,\n",
       "        [  58.14723  ,  -56.004196 ,   86.285164 , ...,  -84.40123  ,\n",
       "           84.80968  , -100.70467  ],\n",
       "        [  66.39647  ,  -51.849937 ,   97.117775 , ...,  -88.897766 ,\n",
       "           92.31235  , -101.10488  ],\n",
       "        [  66.11     ,  -52.419735 ,   94.67511  , ...,  -86.46298  ,\n",
       "           89.63176  ,  -99.49088  ]], dtype=float32),\n",
       " 0,\n",
       " [40, 40000, 40614])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_ds.datasets[1][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": "Shape mismatch, 614 != 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[318], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg16\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     13\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/einops/layers/torch.py:15\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     14\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multirecipe[\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim]\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_for_scriptable_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_axes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/einops/_torch_specific.py:88\u001b[0m, in \u001b[0;36mapply_for_scriptable_torch\u001b[0;34m(recipe, tensor, reduction_type, axes_dims)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_for_scriptable_torch\u001b[39m(\n\u001b[1;32m     78\u001b[0m     recipe: TransformRecipe, tensor: torch\u001b[38;5;241m.\u001b[39mTensor, reduction_type: \u001b[38;5;28mstr\u001b[39m, axes_dims: List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]\n\u001b[1;32m     79\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     80\u001b[0m     backend \u001b[38;5;241m=\u001b[39m TorchJitBackend\n\u001b[1;32m     81\u001b[0m     (\n\u001b[1;32m     82\u001b[0m         init_shapes,\n\u001b[1;32m     83\u001b[0m         axes_reordering,\n\u001b[1;32m     84\u001b[0m         reduced_axes,\n\u001b[1;32m     85\u001b[0m         added_axes,\n\u001b[1;32m     86\u001b[0m         final_shapes,\n\u001b[1;32m     87\u001b[0m         n_axes_w_added,\n\u001b[0;32m---> 88\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct_from_shape_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m init_shapes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[0;32m~/Python/EEG-Dash-Data/.venv/lib/python3.11/site-packages/einops/einops.py:183\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape, axes_dims)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unknown_axes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(length, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(known_product, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m!=\u001b[39m known_product:\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlength\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mknown_product\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# assert len(unknown_axes) == 1, 'this is enforced when recipe is created, so commented out'\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(length, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(known_product, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m length \u001b[38;5;241m%\u001b[39m known_product \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mEinopsError\u001b[0m: Shape mismatch, 614 != 1"
     ]
    }
   ],
   "source": [
    "# Assuming 'test_loader' is your DataLoader for the test dataset\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for data, target in test_loader2:\n",
    "        # move to device\n",
    "        data, target = data.to(device), target.to(device)  \n",
    "        data = normalize_data(data)\n",
    "        if model_name == 'vgg16' or model_name == 'original':\n",
    "            data = data.reshape(data.shape[0], 1, data.shape[1], data.shape[2])\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss      lr     dur\n",
      "-------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6690\u001b[0m       \u001b[32m10.9828\u001b[0m  0.0006  0.6172\n",
      "      2            0.6690       \u001b[32m10.3273\u001b[0m  0.0000  0.4974\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>&lt;class &#x27;braindecode.classifier.EEGClassifier&#x27;&gt;[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 129, 614]             [1, 2]                    --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 129, 614]             [1, 129, 614, 1]          --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 129, 614, 1]          [1, 1, 614, 129]          --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 614, 129]          [1, 40, 590, 1]           207,440                   --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 40, 590, 1]           [1, 40, 590, 1]           80                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 40, 590, 1]           [1, 40, 590, 1]           --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 40, 590, 1]           [1, 40, 35, 1]            --                        [75, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 40, 35, 1]            [1, 2]                    --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 40, 35, 1]            [1, 2, 1, 1]              2,802                     [35, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 210,322\n",
       "  Trainable params: 210,322\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (Units.MEGABYTES): 0.00\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.32\n",
       "  Forward/backward pass size (MB): 0.19\n",
       "  Params size (MB): 0.01\n",
       "  Estimated Total Size (MB): 0.52\n",
       "  ============================================================================================================================================,\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>EEGClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>&lt;class &#x27;braindecode.classifier.EEGClassifier&#x27;&gt;[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 129, 614]             [1, 2]                    --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 129, 614]             [1, 129, 614, 1]          --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 129, 614, 1]          [1, 1, 614, 129]          --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 614, 129]          [1, 40, 590, 1]           207,440                   --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 40, 590, 1]           [1, 40, 590, 1]           80                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 40, 590, 1]           [1, 40, 590, 1]           --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 40, 590, 1]           [1, 40, 35, 1]            --                        [75, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 40, 35, 1]            [1, 2]                    --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 40, 35, 1]            [1, 2, 1, 1]              2,802                     [35, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 210,322\n",
       "  Trainable params: 210,322\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (Units.MEGABYTES): 0.00\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.32\n",
       "  Forward/backward pass size (MB): 0.19\n",
       "  Params size (MB): 0.01\n",
       "  Estimated Total Size (MB): 0.52\n",
       "  ============================================================================================================================================,\n",
       ")</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 129, 614]             [1, 2]                    --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 129, 614]             [1, 129, 614, 1]          --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 129, 614, 1]          [1, 1, 614, 129]          --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 614, 129]          [1, 40, 590, 1]           207,440                   --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 40, 590, 1]           [1, 40, 590, 1]           80                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 40, 590, 1]           [1, 40, 590, 1]           --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 40, 590, 1]           [1, 40, 35, 1]            --                        [75, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 40, 35, 1]            [1, 40, 35, 1]            --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 40, 35, 1]            [1, 2]                    --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 40, 35, 1]            [1, 2, 1, 1]              2,802                     [35, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 210,322\n",
       "  Trainable params: 210,322\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (Units.MEGABYTES): 0.00\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.32\n",
       "  Forward/backward pass size (MB): 0.19\n",
       "  Params size (MB): 0.01\n",
       "  Estimated Total Size (MB): 0.52\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "if model_name in ['vgg16', 'original']:\n",
    "    dataset = train_ds2 # does not work with the model below\n",
    "else:\n",
    "    dataset = train_ds\n",
    "\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 2\n",
    "  \n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=None,\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    "    classes=classes,\n",
    "    dataset=dataset,\n",
    "    max_epochs=n_epochs,\n",
    ")\n",
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset.\n",
    "clf.fit(train_ds,y=np.array(train_ds.get_metadata()['target']))\n",
    "\n",
    "# evaluated the model after training\n",
    "# y_test = test_ds.get_metadata().target\n",
    "# test_acc = clf.score(test_ds, y=y_test)\n",
    "# print(f\"Test acc: {(test_acc * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[210], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m train_val_split \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# By setting n_jobs=-1, cross-validation is performed\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# with all the processors, in this case the output of the training\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# process is not printed sequentially\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m---> 34\u001b[0m     clf, \u001b[43mX_train\u001b[49m, y_train, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv\u001b[38;5;241m=\u001b[39mtrain_val_split, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(cv_results\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(cv_results\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from skorch.callbacks import LRScheduler\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "lr = 0.0625 * 0.01\n",
    "weight_decay = 0\n",
    "batch_size = 64\n",
    "n_epochs = 2\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=None,\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\",\n",
    "        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    "    classes=classes,\n",
    "    max_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "train_val_split = KFold(n_splits=5, shuffle=False)\n",
    "# By setting n_jobs=-1, cross-validation is performed\n",
    "# with all the processors, in this case the output of the training\n",
    "# process is not printed sequentially\n",
    "cv_results = cross_val_score(\n",
    "    clf, train_ds, test_ds, scoring=\"accuracy\", cv=train_val_split, n_jobs=1\n",
    ")\n",
    "print(\n",
    "    f\"Validation accuracy: {np.mean(cv_results * 100):.2f}\"\n",
    "    f\"+-{np.std(cv_results * 100):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 129, 614)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_ds2[0][0]. shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_ds.get_metadata()['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(model, train_loader, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, optimizer, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('M', 'F')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gender_to_label \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgender_to_label\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('M', 'F')"
     ]
    }
   ],
   "source": [
    "gender_to_label = {'M': 0, 'F': 1}\n",
    "gender_to_label['M', 'F']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
