{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegdash import EEGDash\n",
    "import torch\n",
    "import torchvision.models as torchmodels\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use EEGDash to find data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Found 18 records\n",
      "Found 18 records\n",
      "<xarray.DataArray 'eeg_signal__ds002718_sub-014_task-FaceRecognition_eeg.set' (\n",
      "                                                                               channel: 74,\n",
      "                                                                               time: 742500)> Size: 440MB\n",
      "[54945000 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * channel  (channel) object 592B 'EEG001' 'EEG002' ... 'EEG073' 'EEG074'\n",
      "  * time     (time) float64 6MB 0.0 0.004 0.008 ... 2.97e+03 2.97e+03 2.97e+03\n",
      "Attributes:\n",
      "    data_name:           ds002718_sub-014_task-FaceRecognition_eeg.set\n",
      "    dataset:             ds002718\n",
      "    has_file:            True\n",
      "    modality:            EEG\n",
      "    run:                 \n",
      "    sampling_frequency:  250\n",
      "    schema_ref:          eeg_signal\n",
      "    session:             \n",
      "    subject:             14\n",
      "    task:                FaceRecognition\n",
      "    version_timestamp:   0\n",
      "Shape of one array recording data (74, 742500)\n"
     ]
    }
   ],
   "source": [
    "EEGDashInstance = EEGDash()\n",
    "EEGDashInstance.find({'task': 'FaceRecognition'})\n",
    "records = EEGDashInstance.get({'task': 'FaceRecognition'})\n",
    "print(records[0])\n",
    "print('Shape of one array recording data', records[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify PyTorch Dataset and Deep Learning Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGSSL(nn.Module):\n",
    "    def __init__(self, model_params=None):\n",
    "        super().__init__()\n",
    "        default_params = {\n",
    "            'task': 'RP',\n",
    "            'weights': 'DEFAULT'\n",
    "        }\n",
    "\n",
    "        if model_params:\n",
    "            default_params.update(model_params)\n",
    "        for k,v in default_params.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.model: nn.Module = None\n",
    "        self.projection: nn.Linear = None\n",
    "        vgg = self.create_vgg_rescaled(weights=self.weights)\n",
    "        self.encoder = nn.Sequential(vgg.features, vgg.flatten)\n",
    "        \n",
    "    def create_vgg_rescaled(self, subsample=4, feature='raw', weights='DEFAULT'):\n",
    "        tmp = torchmodels.vgg16(weights=weights)\n",
    "        tmp.features = tmp.features[0:17]\n",
    "        vgg16_rescaled = nn.Sequential()\n",
    "        modules = []\n",
    "        \n",
    "        if feature == 'raw':\n",
    "            first_in_channels = 1\n",
    "            first_in_features = 6144\n",
    "        else:\n",
    "            first_in_channels = 3\n",
    "            first_in_features = 576\n",
    "            \n",
    "        for layer in tmp.features.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if layer.in_channels == 3:\n",
    "                    in_channels = first_in_channels\n",
    "                else:\n",
    "                    in_channels = int(layer.in_channels/subsample)\n",
    "                out_channels = int(layer.out_channels/subsample)\n",
    "                modules.append(nn.Conv2d(in_channels, out_channels, layer.kernel_size, layer.stride, layer.padding))\n",
    "            else:\n",
    "                modules.append(layer)\n",
    "        vgg16_rescaled.add_module('features',nn.Sequential(*modules))\n",
    "        vgg16_rescaled.add_module('flatten', nn.Flatten())\n",
    "\n",
    "        modules = []\n",
    "        for layer in tmp.classifier.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if layer.in_features == 25088:\n",
    "                    in_features = first_in_features\n",
    "                else:\n",
    "                    in_features = int(layer.in_features/subsample) \n",
    "                if layer.out_features == 1000:\n",
    "                    out_features = 2\n",
    "                else:\n",
    "                    out_features = int(layer.out_features/subsample) \n",
    "                modules.append(nn.Linear(in_features, out_features))\n",
    "            else:\n",
    "                modules.append(layer)\n",
    "        vgg16_rescaled.add_module('classifier', nn.Sequential(*modules))\n",
    "        return vgg16_rescaled\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @param x: (batch_size, channel, time)\n",
    "        '''\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return self.encode(x)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def aggregate(self, x):\n",
    "        return super().aggregate(x)\n",
    "\n",
    "class DeepLearningEEGDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, records, participants):\n",
    "        self.records = records\n",
    "        self.window_size = 2 # seconds\n",
    "        participants = pd.read_csv(participants, sep='\\t')\n",
    "        self.labels = self.get_labels(participants)\n",
    "\n",
    "    def get_labels(self, participants):\n",
    "        subjects_str = participants['participant_id'].values\n",
    "        subjects_str = [s.split('-')[1] for s in subjects_str]\n",
    "        # get the number removing leading 0s\n",
    "        subjects = [int(s) for s in subjects_str]\n",
    "        gender = participants['gender'].values\n",
    "        def gender_to_int(g):\n",
    "            if g == 'M':\n",
    "                return 0\n",
    "            elif g == 'F':\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        gender_int = list(map(gender_to_int, gender))\n",
    "        labels = dict(zip(subjects, gender_int))\n",
    "        return labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        # set up multi-processing\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = 0\n",
    "            iter_end = len(self.records)\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil(len(self.records) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.records))\n",
    "            print(f'worker_id: {worker_id}, iter_start: {iter_start}, iter_end: {iter_end}\\n')\n",
    "        for i in range(iter_start, iter_end):\n",
    "            record = self.records[i]\n",
    "            data = record.values # C x T    \n",
    "            window_size_in_samples = int(self.window_size * record.sampling_frequency)\n",
    "            indices = np.arange(0, data.shape[1]-window_size_in_samples, window_size_in_samples)\n",
    "            for idx in indices:\n",
    "                if idx < data.shape[-1]-window_size_in_samples:\n",
    "                    yield data[:,idx:idx+window_size_in_samples], self.labels[record.subject]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training loop and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Iter 0 - Loss/train: 10.484759330749512\n",
      "Epoch 0 - Iter 1 - Loss/train: 10.42156982421875\n",
      "Epoch 0 - Iter 2 - Loss/train: 10.353891372680664\n",
      "Epoch 0 - Iter 3 - Loss/train: 10.252344131469727\n",
      "Epoch 0 - Iter 4 - Loss/train: 10.10466480255127\n",
      "Epoch 0 - Iter 5 - Loss/train: 9.891341209411621\n",
      "Epoch 0 - Iter 6 - Loss/train: 9.588701248168945\n",
      "Epoch 0 - Iter 7 - Loss/train: 9.173178672790527\n",
      "Epoch 0 - Iter 8 - Loss/train: 8.620638847351074\n",
      "Epoch 0 - Iter 9 - Loss/train: 7.925732135772705\n",
      "Epoch 0 - Iter 10 - Loss/train: 7.215014934539795\n",
      "Epoch 0 - Iter 11 - Loss/train: 6.938803195953369\n",
      "Epoch 0 - Iter 12 - Loss/train: 7.031151294708252\n",
      "Epoch 0 - Iter 13 - Loss/train: 7.103455066680908\n",
      "Epoch 0 - Iter 14 - Loss/train: 7.073859691619873\n",
      "Epoch 0 - Iter 15 - Loss/train: 6.936770915985107\n",
      "Epoch 0 - Iter 16 - Loss/train: 6.712206840515137\n",
      "Epoch 0 - Iter 17 - Loss/train: 6.441262245178223\n",
      "Epoch 0 - Iter 18 - Loss/train: 6.155370712280273\n",
      "Epoch 0 - Iter 19 - Loss/train: 5.874186038970947\n",
      "Epoch 0 - Iter 20 - Loss/train: 5.61425256729126\n"
     ]
    }
   ],
   "source": [
    "dataset = DeepLearningEEGDataset(records, 'participants.tsv')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=0)\n",
    "model = VGGSSL()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.002, weight_decay=0.001)\n",
    "model.train()\n",
    "for e in range(1):\n",
    "    for t, (samples, labels) in enumerate(dataloader):\n",
    "        samples = samples.to(dtype=torch.float32)\n",
    "        labels = labels.to(dtype=torch.long)\n",
    "        scores = model(samples)\n",
    "        loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {e} - Iter {t} - Loss/train: {loss.item()}\")\n",
    "        if t == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
