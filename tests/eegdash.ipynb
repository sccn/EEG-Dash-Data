{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGDash example\n",
    "\n",
    "The code below provides an example of using the *EEGDash* library in combination with PyTorch to develop a deep learning model for analyzing EEG data, specifically for the task of gender discrimination.\n",
    "\n",
    "1. **Library Imports**: Essential libraries are imported, including *EEGDash* for data handling, *torch* for deep learning model definition, and other utilities such as *xarray*, *pandas*, and *numpy*.\n",
    "\n",
    "2. **Data Retrieval Using EEGDash**: An instance of *EEGDash* is created to search and retrieve an EEG dataset (in this case \"FaceRecognition\"). Then data records are fetched and inspected.\n",
    "\n",
    "3. **Model Definition**: A custom deep learning model (*VGGSSL*) based on a rescaled version of the VGG-16 model is defined, intended to encode EEG signal data [1]. The model structure includes: (1) **VGG16 Rescaling**, the convolutional layers are modified to fit the specific dimensions of EEG datam and (2) **Encoding Layer**, an encoding layer allows feature extraction suitable for downstream tasks.\n",
    "\n",
    "4. **Custom Dataset Class**: *DeepLearningEEGDataset* is defined to structure the EEG data in a PyTorch-compatible format, including reading participant labels (e.g., gender) from a *participants.tsv* file. This class assigns labels to each subject and plits EEG recordings into smaller time windows for better input handling.\n",
    "\n",
    "5. **Training Loop**: The dataset is loaded into a PyTorch DataLoader. A training loop is initiated to perform backpropagation and train the model and loss is printed periodically to track model performance.\n",
    "\n",
    "Refrences:\n",
    "\n",
    "[1] Truong, D., Milham, M., Makeig, S., & Delorme, A. (2021). Deep Convolutional Neural Network Applied to Electroencephalography: Raw Data vs Spectral Features. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2021, 1039â€“1042. https://doi.org/10.1109/EMBC46164.2021.9630708\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegdash import EEGDash\n",
    "import torch\n",
    "import torchvision.models as torchmodels\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use EEGDash to find data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Found 18 records\n",
      "Found 18 records\n",
      "<xarray.DataArray 'eeg_signal__ds002718_sub-014_task-FaceRecognition_eeg.set' (\n",
      "                                                                               channel: 74,\n",
      "                                                                               time: 742500)> Size: 440MB\n",
      "[54945000 values with dtype=float64]\n",
      "Coordinates:\n",
      "  * channel  (channel) object 592B 'EEG001' 'EEG002' ... 'EEG073' 'EEG074'\n",
      "  * time     (time) float64 6MB 0.0 0.004 0.008 ... 2.97e+03 2.97e+03 2.97e+03\n",
      "Attributes:\n",
      "    data_name:           ds002718_sub-014_task-FaceRecognition_eeg.set\n",
      "    dataset:             ds002718\n",
      "    has_file:            True\n",
      "    modality:            EEG\n",
      "    run:                 \n",
      "    sampling_frequency:  250\n",
      "    schema_ref:          eeg_signal\n",
      "    session:             \n",
      "    subject:             14\n",
      "    task:                FaceRecognition\n",
      "    version_timestamp:   0\n",
      "Shape of one array recording data (74, 742500)\n"
     ]
    }
   ],
   "source": [
    "EEGDashInstance = EEGDash()\n",
    "EEGDashInstance.find({'task': 'FaceRecognition'})\n",
    "records = EEGDashInstance.get({'task': 'FaceRecognition'})\n",
    "print(records[0])\n",
    "print('Shape of one array recording data', records[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify PyTorch Dataset and Deep Learning Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGSSL(nn.Module):\n",
    "    def __init__(self, model_params=None):\n",
    "        super().__init__()\n",
    "        default_params = {\n",
    "            'task': 'RP',\n",
    "            'weights': 'DEFAULT'\n",
    "        }\n",
    "\n",
    "        if model_params:\n",
    "            default_params.update(model_params)\n",
    "        for k,v in default_params.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.model: nn.Module = None\n",
    "        self.projection: nn.Linear = None\n",
    "        vgg = self.create_vgg_rescaled(weights=self.weights)\n",
    "        self.encoder = nn.Sequential(vgg.features, vgg.flatten)\n",
    "        \n",
    "    def create_vgg_rescaled(self, subsample=4, feature='raw', weights='DEFAULT'):\n",
    "        tmp = torchmodels.vgg16(weights=weights)\n",
    "        tmp.features = tmp.features[0:17]\n",
    "        vgg16_rescaled = nn.Sequential()\n",
    "        modules = []\n",
    "        \n",
    "        if feature == 'raw':\n",
    "            first_in_channels = 1\n",
    "            first_in_features = 6144\n",
    "        else:\n",
    "            first_in_channels = 3\n",
    "            first_in_features = 576\n",
    "            \n",
    "        for layer in tmp.features.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if layer.in_channels == 3:\n",
    "                    in_channels = first_in_channels\n",
    "                else:\n",
    "                    in_channels = int(layer.in_channels/subsample)\n",
    "                out_channels = int(layer.out_channels/subsample)\n",
    "                modules.append(nn.Conv2d(in_channels, out_channels, layer.kernel_size, layer.stride, layer.padding))\n",
    "            else:\n",
    "                modules.append(layer)\n",
    "        vgg16_rescaled.add_module('features',nn.Sequential(*modules))\n",
    "        vgg16_rescaled.add_module('flatten', nn.Flatten())\n",
    "\n",
    "        modules = []\n",
    "        for layer in tmp.classifier.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                if layer.in_features == 25088:\n",
    "                    in_features = first_in_features\n",
    "                else:\n",
    "                    in_features = int(layer.in_features/subsample) \n",
    "                if layer.out_features == 1000:\n",
    "                    out_features = 2\n",
    "                else:\n",
    "                    out_features = int(layer.out_features/subsample) \n",
    "                modules.append(nn.Linear(in_features, out_features))\n",
    "            else:\n",
    "                modules.append(layer)\n",
    "        vgg16_rescaled.add_module('classifier', nn.Sequential(*modules))\n",
    "        return vgg16_rescaled\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @param x: (batch_size, channel, time)\n",
    "        '''\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        return self.encode(x)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def aggregate(self, x):\n",
    "        return super().aggregate(x)\n",
    "\n",
    "class DeepLearningEEGDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, records, participants):\n",
    "        self.records = records\n",
    "        self.window_size = 2 # seconds\n",
    "        participants = pd.read_csv(participants, sep='\\t')\n",
    "        self.labels = self.get_labels(participants)\n",
    "\n",
    "    def get_labels(self, participants):\n",
    "        subjects_str = participants['participant_id'].values\n",
    "        subjects_str = [s.split('-')[1] for s in subjects_str]\n",
    "        # get the number removing leading 0s\n",
    "        subjects = [int(s) for s in subjects_str]\n",
    "        gender = participants['gender'].values\n",
    "        def gender_to_int(g):\n",
    "            if g == 'M':\n",
    "                return 0\n",
    "            elif g == 'F':\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        gender_int = list(map(gender_to_int, gender))\n",
    "        labels = dict(zip(subjects, gender_int))\n",
    "        return labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        # set up multi-processing\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # single-process data loading, return the full iterator\n",
    "            iter_start = 0\n",
    "            iter_end = len(self.records)\n",
    "        else:  # in a worker process\n",
    "            # split workload\n",
    "            per_worker = int(math.ceil(len(self.records) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.records))\n",
    "            print(f'worker_id: {worker_id}, iter_start: {iter_start}, iter_end: {iter_end}\\n')\n",
    "        for i in range(iter_start, iter_end):\n",
    "            record = self.records[i]\n",
    "            data = record.values # C x T    \n",
    "            window_size_in_samples = int(self.window_size * record.sampling_frequency)\n",
    "            indices = np.arange(0, data.shape[1]-window_size_in_samples, window_size_in_samples)\n",
    "            for idx in indices:\n",
    "                if idx < data.shape[-1]-window_size_in_samples:\n",
    "                    yield data[:,idx:idx+window_size_in_samples], self.labels[record.subject]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training loop and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/arno/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528M/528M [00:15<00:00, 34.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Iter 0 - Loss/train: 10.462847709655762\n",
      "Epoch 0 - Iter 1 - Loss/train: 10.416534423828125\n",
      "Epoch 0 - Iter 2 - Loss/train: 10.368654251098633\n",
      "Epoch 0 - Iter 3 - Loss/train: 10.308568000793457\n",
      "Epoch 0 - Iter 4 - Loss/train: 10.228968620300293\n",
      "Epoch 0 - Iter 5 - Loss/train: 10.12086296081543\n",
      "Epoch 0 - Iter 6 - Loss/train: 9.975360870361328\n",
      "Epoch 0 - Iter 7 - Loss/train: 9.777841567993164\n",
      "Epoch 0 - Iter 8 - Loss/train: 9.512860298156738\n",
      "Epoch 0 - Iter 9 - Loss/train: 9.160260200500488\n",
      "Epoch 0 - Iter 10 - Loss/train: 8.696739196777344\n",
      "Epoch 0 - Iter 11 - Loss/train: 8.117931365966797\n",
      "Epoch 0 - Iter 12 - Loss/train: 7.482291221618652\n",
      "Epoch 0 - Iter 13 - Loss/train: 7.119117736816406\n",
      "Epoch 0 - Iter 14 - Loss/train: 7.198959827423096\n",
      "Epoch 0 - Iter 15 - Loss/train: 7.317335605621338\n",
      "Epoch 0 - Iter 16 - Loss/train: 7.335928440093994\n",
      "Epoch 0 - Iter 17 - Loss/train: 7.238356113433838\n",
      "Epoch 0 - Iter 18 - Loss/train: 7.0485334396362305\n",
      "Epoch 0 - Iter 19 - Loss/train: 6.806842803955078\n",
      "Epoch 0 - Iter 20 - Loss/train: 6.546984672546387\n"
     ]
    }
   ],
   "source": [
    "dataset = DeepLearningEEGDataset(records, 'participants.tsv')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=0)\n",
    "model = VGGSSL()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=0.002, weight_decay=0.001)\n",
    "model.train()\n",
    "for e in range(1):\n",
    "    for t, (samples, labels) in enumerate(dataloader):\n",
    "        samples = samples.to(dtype=torch.float32)\n",
    "        labels = labels.to(dtype=torch.long)\n",
    "        scores = model(samples)\n",
    "        loss = F.cross_entropy(scores, labels)\n",
    "\n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "\n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {e} - Iter {t} - Loss/train: {loss.item()}\")\n",
    "        if t == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
