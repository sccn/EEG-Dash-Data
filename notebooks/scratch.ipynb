{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from eegdash import EEGDash, EEGDashDataset\n",
    "from eegdash.data_utils import EEGDashBaseRaw\n",
    "\n",
    "eegdashdata = EEGDash(is_public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-014/eeg/sub-014_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-012/eeg/sub-012_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-015/eeg/sub-015_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-008/eeg/sub-008_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-006/eeg/sub-006_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-013/eeg/sub-013_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-004/eeg/sub-004_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-016/eeg/sub-016_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-002/eeg/sub-002_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-018/eeg/sub-018_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-007/eeg/sub-007_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-010/eeg/sub-010_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-017/eeg/sub-017_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-003/eeg/sub-003_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-011/eeg/sub-011_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-005/eeg/sub-005_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-009/eeg/sub-009_task-FaceRecognition_eeg.set\n",
      "bids raw file /mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.set\n"
     ]
    }
   ],
   "source": [
    "# eegdash_braindecode = EEGDashDataset({'dataset': 'ds005509', 'task': 'RestingState'},\n",
    "#                            description_fields=['age', 'sex', 'subject'])\n",
    "eegdash_braindecode = EEGDashDataset(dataset='ds002718', data_dir='/mnt/nemar/openneuro/ds002718', description_fields=['age', 'gender', 'subject'])\n",
    "target_name = 'sex'\n",
    "for ds in eegdash_braindecode.datasets:\n",
    "    ds.target_name = target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.0</td>\n",
       "      <td>F</td>\n",
       "      <td>014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>M</td>\n",
       "      <td>012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>M</td>\n",
       "      <td>015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>F</td>\n",
       "      <td>008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24.0</td>\n",
       "      <td>F</td>\n",
       "      <td>016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.0</td>\n",
       "      <td>M</td>\n",
       "      <td>002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25.0</td>\n",
       "      <td>F</td>\n",
       "      <td>018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31.0</td>\n",
       "      <td>F</td>\n",
       "      <td>010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.0</td>\n",
       "      <td>F</td>\n",
       "      <td>017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25.0</td>\n",
       "      <td>M</td>\n",
       "      <td>003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26.0</td>\n",
       "      <td>M</td>\n",
       "      <td>011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>30.0</td>\n",
       "      <td>M</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26.0</td>\n",
       "      <td>M</td>\n",
       "      <td>009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age gender subject\n",
       "0   26.0      F     014\n",
       "1   29.0      M     012\n",
       "2   23.0      M     015\n",
       "3   23.0      F     008\n",
       "4    NaN    NaN     006\n",
       "5    NaN    NaN     013\n",
       "6    NaN    NaN     004\n",
       "7   24.0      F     016\n",
       "8   31.0      M     002\n",
       "9   25.0      F     018\n",
       "10   NaN    NaN     007\n",
       "11  31.0      F     010\n",
       "12  24.0      F     017\n",
       "13  25.0      M     003\n",
       "14  26.0      M     011\n",
       "15  30.0      M     005\n",
       "16  26.0      M     009\n",
       "17   NaN    NaN     019"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eegdash_braindecode.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from eegdash.data_utils import BIDSDataset\n",
    "bidsdataset = BIDSDataset('/mnt/nemar/openneuro', 'ds002718')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/nemar/openneuro/ds002718/sub-014/eeg/sub-014_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-012/eeg/sub-012_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-015/eeg/sub-015_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-008/eeg/sub-008_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-006/eeg/sub-006_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-013/eeg/sub-013_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-004/eeg/sub-004_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-016/eeg/sub-016_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-002/eeg/sub-002_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-018/eeg/sub-018_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-007/eeg/sub-007_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-010/eeg/sub-010_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-017/eeg/sub-017_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-003/eeg/sub-003_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-011/eeg/sub-011_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-005/eeg/sub-005_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-009/eeg/sub-009_task-FaceRecognition_eeg.set',\n",
       "       '/mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.set'],\n",
       "      dtype='<U78')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsdataset.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TaskName': 'FaceRecognition',\n",
       " 'TaskDescription': \"Subjects viewed stimuli on a screen during six, 7.5 minute runs. The stimuli were photographs of either a famous face (known to most of British or a scrambled face, and appeared for a random duration between 800 and 1,000 ms. Subjects were instructed to fixate centrally throughout the experiment. To ensure attention to each stimulus, participants were asked to press one of two keys with either their left or right index finger (assignment counter-balanced across participants). Their key-press was based on how symmetric they regarded each image: pressing one or the other key depending whether they thought the image was 'more' or 'less symmetric' than average.\",\n",
       " 'InstitutionAddress': '15 Chaucer Road, Cambridge, UK',\n",
       " 'InstitutionName': 'MRC Cognition & Brain Sciences Unit',\n",
       " 'EEGReference': 'nose',\n",
       " 'EEGGround': 'left collar bone',\n",
       " 'SamplingFrequency': 250,\n",
       " 'PowerLineFrequency': 50,\n",
       " 'SoftwareFilters': {'LowPassFilter': {'cutoff': '350 (Hz)'}},\n",
       " 'EEGPlacementScheme': 'extended 10-10% system',\n",
       " 'CapManufacturer': 'Easycap',\n",
       " 'EEGChannelCount': 70,\n",
       " 'EOGChannelCount': 2,\n",
       " 'RecordingType': 'continuous',\n",
       " 'MiscChannelCount': 4,\n",
       " 'RecordingDuration': 2973,\n",
       " 'ECGChannelCount': 0,\n",
       " 'EMGChannelCount': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsdataset.eeg_json('/mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.json')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsdataset.get_bids_metadata_files('/mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.set', 'eeg.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/nemar/openneuro/ds002718/participants.tsv')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidsdataset.get_bids_metadata_files('/mnt/nemar/openneuro/ds002718/sub-019/eeg/sub-019_task-FaceRecognition_eeg.set', 'participants.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
